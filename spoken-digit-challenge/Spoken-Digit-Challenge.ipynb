{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoken Digit Challenge\n",
    "\n",
    "This is the first challenge of our Speech and Machine Learning Workshop. Here we will use the [FSDD][] Free Spoken Digit Dataset to build different models and recognize the digits from speech.   \n",
    "\n",
    "** Note: ** Make sure that your dataset is in the correct folder - if there´s something not working for you, feel free to ask.\n",
    "\n",
    "* 1500 recordings in total (150 per digit)\n",
    "* 8kHz sampling rate\n",
    "* 3 speakers\n",
    "* English \n",
    "* File format: {digit\\_label}\\_{speaker\\_name}\\_{index}.wav <br> (e.g. \"4\\_jackson\\_16.wav\")\n",
    "\n",
    "[FSDD]: https://github.com/Jakobovski/free-spoken-digit-dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "First, we will extract our features from the audio files. Two files will be generated - one for the features and one for the corresponding labels. Each line in our feature-label-pair will represent a single audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant modules to be used later\n",
    "import glob\n",
    "import os\n",
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "\n",
    "\n",
    "\n",
    "# Config matplotlib for inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "DATASET_DIR = \"dataset/\"\n",
    "\n",
    "# Create a list of all .wav files in the dataset directoy paths \n",
    "sound_paths = [DATASET_DIR + f for f in os.listdir(DATASET_DIR) if f[-4:] == '.wav' and 'jackson' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sound_files(file_paths):\n",
    "    return [librosa.load(fp)[0] for fp in file_paths]\n",
    "\n",
    "def plot_wave(sound_name_with_raw_data):\n",
    "    i = 1\n",
    "    plt.figure(figsize=(15, 2 * len(sound_name_with_raw_data) if len(sound_name_with_raw_data) > 1 else 4))\n",
    "    for n,d in sound_name_with_raw_data:\n",
    "        plt.subplot(np.ceil(float(len(sound_name_with_raw_data))/2), 2, i)\n",
    "        \n",
    "        # wave plot\n",
    "        librosa.display.waveplot(np.array(d),sr=8000)\n",
    "        \n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.title(n)\n",
    "        i += 1\n",
    "    plt.subplots_adjust(top=0.8, bottom=0.08, left=0.10, right=0.95, hspace=0.5, wspace=0.35)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_spectrogram(sound_name_with_raw_data):\n",
    "    i = 1\n",
    "    plt.figure(figsize=(15, 2 * len(sound_name_with_raw_data) if len(sound_name_with_raw_data) > 1 else 4))\n",
    "    for n,d in sound_name_with_raw_data:\n",
    "        plt.subplot(np.ceil(float(len(sound_name_with_raw_data))/2), 2, i)\n",
    "        \n",
    "        # Spectrogram\n",
    "        specgram(np.array(d), Fs=8000, NFFT=512, noverlap=248, scale=\"dB\", vmax=20)\n",
    "        \n",
    "        plt.title(n)\n",
    "        i += 1\n",
    "    plt.subplots_adjust(top=0.8, bottom=0.08, left=0.10, right=0.95, hspace=0.5, wspace=0.35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target sound filenames for visualization\n",
    "sound_filenames = [str(i) + '_jackson_0.wav' for i in range(0, 10)]\n",
    "\n",
    "# Load sound files used in visualization\n",
    "sound_name_with_raw_data = [(\"Digit \" + os.path.basename(p)[0], librosa.load(p)[0]) for i, p in enumerate(sound_paths) if os.path.basename(p) in sound_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_wave(sound_name_with_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_spectrogram(sound_name_with_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    return mfccs,chroma,mel,contrast,tonnetz\n",
    "\n",
    "def get_features_and_labels(sound_paths):\n",
    "    features, labels = np.empty((0,193)), np.empty(0)\n",
    "    for p in sound_paths:\n",
    "        mfccs, chroma, mel, contrast,tonnetz = extract_feature(p)\n",
    "        ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "        features = np.vstack([features,ext_features])\n",
    "        labels = np.append(labels, int(os.path.basename(p)[0]))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = get_features_and_labels(sound_paths)\n",
    "labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURE_PATH = 'features/features.txt'\n",
    "LABEL_PATH = 'features/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(FEATURE_PATH, features, fmt='%10.5f', delimiter='\\t')\n",
    "np.savetxt(LABEL_PATH, labels, fmt='%i', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification\n",
    "\n",
    "Now, we will load our generated features and labels in order to train a classifier on it and evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from keras.initializers import lecun_uniform\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: (500, 10)\n",
      "Feature dimensions: 193\n"
     ]
    }
   ],
   "source": [
    "features = np.loadtxt(FEATURE_PATH)\n",
    "labels = np.loadtxt(LABEL_PATH)\n",
    "\n",
    "print('Label shape: ' + str(labels.shape))\n",
    "feature_dim = features.shape[1]\n",
    "print('Feature dimensions: ' + str(feature_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total samples: 500\n",
      "Train sample size: 250\n",
      "Test sample size: 150\n",
      "Eval sample size: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_train_test_eval (features, labels, train_percentage, test_percentage, eval_percentage):\n",
    "    feature_label_pairs = list(zip(features, labels))\n",
    "    random.shuffle(feature_label_pairs)\n",
    "    features, labels = zip(*feature_label_pairs)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    sample_size = len(labels)\n",
    "    print('Number of total samples: ' + str(sample_size))\n",
    "    \n",
    "    train_samples = int(sample_size * train_percentage)\n",
    "    test_samples = int(sample_size * test_percentage)\n",
    "    eval_samples = int(sample_size * eval_percentage)\n",
    "    \n",
    "    # just to make sure that we end up with the actual sample size:\n",
    "    if train_samples + test_samples + eval_samples > sample_size:\n",
    "        eval_samples = sample_size - train_samples - test_samples\n",
    "    \n",
    "    print('Train sample size: ' + str(train_samples))\n",
    "    print('Test sample size: ' + str(test_samples))\n",
    "    print('Eval sample size: ' + str(eval_samples))\n",
    "    \n",
    "    train_features = features[0 : train_samples]\n",
    "    train_labels = labels[0 : train_samples]\n",
    "    \n",
    "    test_features = features[train_samples : train_samples + test_samples]\n",
    "    test_labels = labels[train_samples : train_samples + test_samples]\n",
    "    \n",
    "    eval_features = features[train_samples + test_samples : train_samples + test_samples + eval_samples]\n",
    "    eval_labels = labels[train_samples + test_samples : train_samples + test_samples + eval_samples]\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels, eval_features, eval_labels\n",
    "        \n",
    "train_features, train_labels, test_features, test_labels, eval_features, eval_labels = split_train_test_eval (features, labels, 0.5, 0.3, 0.2)\n",
    "    \n",
    "evaluation = (eval_features, eval_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 250 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.6713 - acc: 0.7704 - val_loss: 0.7667 - val_acc: 0.8430\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 276us/step - loss: 0.5088 - acc: 0.8772 - val_loss: 0.6067 - val_acc: 0.8710\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 276us/step - loss: 0.4315 - acc: 0.8868 - val_loss: 0.5660 - val_acc: 0.8700\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.4011 - acc: 0.9008 - val_loss: 0.4690 - val_acc: 0.8800\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.3517 - acc: 0.9108 - val_loss: 0.6526 - val_acc: 0.8230\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.3517 - acc: 0.9112 - val_loss: 0.4717 - val_acc: 0.8940\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.3124 - acc: 0.9288 - val_loss: 0.4429 - val_acc: 0.9020\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.3146 - acc: 0.9244 - val_loss: 0.3904 - val_acc: 0.9110\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.2955 - acc: 0.9352 - val_loss: 0.3828 - val_acc: 0.9230\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.2837 - acc: 0.9344 - val_loss: 0.3961 - val_acc: 0.9100\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.2753 - acc: 0.9372 - val_loss: 0.3696 - val_acc: 0.9250\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.2581 - acc: 0.9416 - val_loss: 0.3327 - val_acc: 0.9280\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.1904 - acc: 0.9400 - val_loss: 0.2089 - val_acc: 0.9280\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.1519 - acc: 0.9428 - val_loss: 0.2094 - val_acc: 0.9180\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.1266 - acc: 0.9524 - val_loss: 0.1810 - val_acc: 0.9330\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.1174 - acc: 0.9540 - val_loss: 0.1700 - val_acc: 0.9300\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.1037 - acc: 0.9604 - val_loss: 0.1662 - val_acc: 0.9350\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.1004 - acc: 0.9636 - val_loss: 0.1373 - val_acc: 0.9450\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0921 - acc: 0.9656 - val_loss: 0.1575 - val_acc: 0.9430\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0907 - acc: 0.9648 - val_loss: 0.1301 - val_acc: 0.9520\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0864 - acc: 0.9692 - val_loss: 0.1535 - val_acc: 0.9440\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0770 - acc: 0.9740 - val_loss: 0.1209 - val_acc: 0.9540\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0816 - acc: 0.9728 - val_loss: 0.2299 - val_acc: 0.9080\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0785 - acc: 0.9696 - val_loss: 0.1337 - val_acc: 0.9520\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0705 - acc: 0.9756 - val_loss: 0.1590 - val_acc: 0.9340\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 276us/step - loss: 0.0706 - acc: 0.9756 - val_loss: 0.1430 - val_acc: 0.9370\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 401us/step - loss: 0.0672 - acc: 0.9764 - val_loss: 0.1214 - val_acc: 0.9510\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 401us/step - loss: 0.0645 - acc: 0.9780 - val_loss: 0.1404 - val_acc: 0.9460\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 313us/step - loss: 0.0652 - acc: 0.9788 - val_loss: 0.1089 - val_acc: 0.9560\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0623 - acc: 0.9796 - val_loss: 0.1189 - val_acc: 0.9530\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 250us/step - loss: 0.0608 - acc: 0.9828 - val_loss: 0.1077 - val_acc: 0.9600\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0574 - acc: 0.9852 - val_loss: 0.1182 - val_acc: 0.9560\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0568 - acc: 0.9848 - val_loss: 0.0990 - val_acc: 0.9620\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0527 - acc: 0.9884 - val_loss: 0.1155 - val_acc: 0.9560\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0520 - acc: 0.9832 - val_loss: 0.1039 - val_acc: 0.9600\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0528 - acc: 0.9844 - val_loss: 0.0965 - val_acc: 0.9670\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0500 - acc: 0.9880 - val_loss: 0.1655 - val_acc: 0.9300\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0518 - acc: 0.9856 - val_loss: 0.1038 - val_acc: 0.9640\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0482 - acc: 0.9876 - val_loss: 0.0945 - val_acc: 0.9670\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0475 - acc: 0.9888 - val_loss: 0.0995 - val_acc: 0.9630\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0452 - acc: 0.9888 - val_loss: 0.1576 - val_acc: 0.9380\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0489 - acc: 0.9872 - val_loss: 0.0996 - val_acc: 0.9650\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0443 - acc: 0.9892 - val_loss: 0.0888 - val_acc: 0.9690\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0436 - acc: 0.9896 - val_loss: 0.0901 - val_acc: 0.9720\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0433 - acc: 0.9900 - val_loss: 0.0864 - val_acc: 0.9770\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0866 - val_acc: 0.9730\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.1010 - val_acc: 0.9630\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 0.0918 - val_acc: 0.9660\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 135us/step - loss: 0.0392 - acc: 0.9920 - val_loss: 0.0945 - val_acc: 0.9640\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0988 - val_acc: 0.9660\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 139us/step - loss: 0.0397 - acc: 0.9904 - val_loss: 0.1080 - val_acc: 0.9610\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0428 - acc: 0.9908 - val_loss: 0.1012 - val_acc: 0.9690\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0377 - acc: 0.9928 - val_loss: 0.1168 - val_acc: 0.9560\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0391 - acc: 0.9920 - val_loss: 0.0836 - val_acc: 0.9770\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0369 - acc: 0.9916 - val_loss: 0.0863 - val_acc: 0.9690\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0373 - acc: 0.9928 - val_loss: 0.0868 - val_acc: 0.9750\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0359 - acc: 0.9924 - val_loss: 0.0843 - val_acc: 0.9720\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0367 - acc: 0.9928 - val_loss: 0.0868 - val_acc: 0.9750\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0353 - acc: 0.9936 - val_loss: 0.0801 - val_acc: 0.9760\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0352 - acc: 0.9924 - val_loss: 0.1048 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0349 - acc: 0.9928 - val_loss: 0.0795 - val_acc: 0.9730\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0329 - acc: 0.9936 - val_loss: 0.0897 - val_acc: 0.9650\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0353 - acc: 0.9916 - val_loss: 0.0859 - val_acc: 0.9710\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0336 - acc: 0.9912 - val_loss: 0.0775 - val_acc: 0.9730\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0330 - acc: 0.9924 - val_loss: 0.0818 - val_acc: 0.9730\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0328 - acc: 0.9920 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.0810 - val_acc: 0.9740\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0324 - acc: 0.9932 - val_loss: 0.1073 - val_acc: 0.9600\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0316 - acc: 0.9940 - val_loss: 0.0788 - val_acc: 0.9760\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0317 - acc: 0.9928 - val_loss: 0.0741 - val_acc: 0.9770\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0308 - acc: 0.9928 - val_loss: 0.0788 - val_acc: 0.9780\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0303 - acc: 0.9932 - val_loss: 0.0760 - val_acc: 0.9770\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0296 - acc: 0.9956 - val_loss: 0.0751 - val_acc: 0.9730\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0293 - acc: 0.9944 - val_loss: 0.0769 - val_acc: 0.9780\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0296 - acc: 0.9936 - val_loss: 0.0734 - val_acc: 0.9750\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0289 - acc: 0.9948 - val_loss: 0.0763 - val_acc: 0.9750\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0292 - acc: 0.9944 - val_loss: 0.1647 - val_acc: 0.9420\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0314 - acc: 0.9940 - val_loss: 0.0971 - val_acc: 0.9660\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0288 - acc: 0.9936 - val_loss: 0.0781 - val_acc: 0.9760\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0273 - acc: 0.9952 - val_loss: 0.0781 - val_acc: 0.9700\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0278 - acc: 0.9944 - val_loss: 0.0725 - val_acc: 0.9760\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0276 - acc: 0.9952 - val_loss: 0.0738 - val_acc: 0.9750\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0283 - acc: 0.9944 - val_loss: 0.0781 - val_acc: 0.9730\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0275 - acc: 0.9932 - val_loss: 0.1053 - val_acc: 0.9550\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0292 - acc: 0.9956 - val_loss: 0.0739 - val_acc: 0.9780\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0281 - acc: 0.9944 - val_loss: 0.0705 - val_acc: 0.9760\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0260 - acc: 0.9952 - val_loss: 0.0720 - val_acc: 0.9760\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0259 - acc: 0.9952 - val_loss: 0.0723 - val_acc: 0.9750\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0255 - acc: 0.9956 - val_loss: 0.0755 - val_acc: 0.9770\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 187us/step - loss: 0.0268 - acc: 0.9948 - val_loss: 0.0685 - val_acc: 0.9780\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0252 - acc: 0.9956 - val_loss: 0.0684 - val_acc: 0.9790\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0250 - acc: 0.9956 - val_loss: 0.0699 - val_acc: 0.9770\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0257 - acc: 0.9952 - val_loss: 0.0758 - val_acc: 0.9770\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0251 - acc: 0.9952 - val_loss: 0.0707 - val_acc: 0.9740\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0242 - acc: 0.9952 - val_loss: 0.0691 - val_acc: 0.9750\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 188us/step - loss: 0.0235 - acc: 0.9968 - val_loss: 0.1047 - val_acc: 0.9640\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0241 - acc: 0.9968 - val_loss: 0.0666 - val_acc: 0.9770\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 151us/step - loss: 0.0235 - acc: 0.9948 - val_loss: 0.0675 - val_acc: 0.9770\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 125us/step - loss: 0.0229 - acc: 0.9968 - val_loss: 0.0731 - val_acc: 0.9730\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 214us/step - loss: 0.0232 - acc: 0.9952 - val_loss: 0.0685 - val_acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=300, input_dim=feature_dim, activation=\"relu\"))\n",
    "model.add(Dense(units=300,activation=\"relu\"))\n",
    "model.add(Dense(units=50, activation=\"relu\"))\n",
    "model.add(Dense(units=10,activation=\"sigmoid\"))\n",
    "\n",
    "opt = optimizers.SGD(lr=0.001, clipvalue=0.5)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_features,train_labels, validation_data=evaluation, epochs=100, batch_size=8)\n",
    "\n",
    "MODEL_DIR = \"models/model1.model\"\n",
    "\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 6 6 0 2 5 2 6 7 9 8 3 6 3 2 1 4 5 5 5 4 7 1 7 0 0 1 2 5 1 7 3 8 3 7\n",
      " 6 4 8 7 2 3 6 1 6 0 8 5 9 3 0 3 5 9 9 1 5 4 9 5 1 6 4 8 3 9 6 9 2 5 2 9 7\n",
      " 3 6 8 5 3 5 4 4 0 8 6 9 8 3 5 1 5 8 7 4 1 8 9 3 7 4 3 5 9 6 3 7 8 8 2 3 0\n",
      " 5 8 9 8 3 5 3 2 2 3 6 7 7 4 6 5 4 4 0 5 8 8 0 9 8 3 9 9 5 1 0 2 0 0 5 1 0\n",
      " 3 5]\n",
      "[7 2 1 6 6 0 2 5 2 6 7 9 8 3 6 2 2 1 4 5 5 5 4 7 1 1 0 0 1 2 5 1 7 3 8 3 7\n",
      " 6 4 8 7 2 3 6 1 6 0 8 5 9 3 0 3 5 9 1 1 5 4 9 5 1 6 4 8 3 1 6 9 2 5 2 9 1\n",
      " 3 6 8 5 3 5 4 4 4 8 6 9 8 3 5 1 5 8 7 4 1 8 9 3 7 4 2 5 9 6 2 7 8 8 3 3 0\n",
      " 5 8 9 8 3 5 3 2 2 3 6 7 7 4 6 5 4 4 0 5 8 8 0 9 8 3 9 9 5 1 0 2 0 0 5 1 0\n",
      " 3 5]\n",
      "Accuracy: 0.94\n",
      "[[12  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  2  0  2]\n",
      " [ 0  0 11  3  0  0  0  0  0  0]\n",
      " [ 0  0  1 17  0  0  0  0  0  0]\n",
      " [ 1  0  0  0 12  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 22  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "prediction_probabilities = np.array(model.predict_proba(test_features))\n",
    "prediction = np.array(model.predict_classes(test_features))\n",
    "                                            \n",
    "test_classes = np.argmax(test_labels, axis=1)\n",
    "print(prediction)\n",
    "print(test_classes)\n",
    "\n",
    "accuracy = accuracy_score(test_classes, prediction)\n",
    "print('Accuracy: ' + str(accuracy))\n",
    "\n",
    "conf_mat = confusion_matrix(test_classes, prediction)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
